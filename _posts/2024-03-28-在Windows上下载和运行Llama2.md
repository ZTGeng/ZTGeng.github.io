---
layout: post
title: åœ¨ Windows ä¸Šä¸‹è½½å’Œè¿è¡Œ Llama2
date: 2024-03-28 22:26:10
---

tl;dr æœ¬æ–‡è®°å½•äº†æˆ‘åœ¨æˆ‘çš„ Windows PC ä¸Šä¸‹è½½ Llama2 æ¨¡å‹å¹¶è¿è¡Œç¤ºä¾‹ä»£ç ï¼ˆ[repo](https://github.com/meta-llama/llama)ï¼‰çš„è¿‡ç¨‹ï¼Œé€‚ç”¨äºè¯¥ repo æˆªè‡³æœ¬æ—¥ï¼ˆ3/28/2024ï¼‰çš„ä»£ç ç‰ˆæœ¬ã€‚

Meta Llama2 æ¨¡å‹çš„[å®˜æ–¹ repo](https://github.com/meta-llama/llama) ä»‹ç»äº†ä¸‹è½½å’Œæœ¬åœ°æµ‹è¯•æ¨¡å‹çš„æ–¹æ³•ï¼Œåªè¦æŒ‰ç…§ readme çš„æŒ‡å¼•ä¸€æ­¥æ­¥æ“ä½œå³å¯ã€‚å¯æ˜¯è¿™ä¸ª repo åªé€‚ç”¨äº Linux ç¯å¢ƒã€‚å¦‚æœä½ æƒ³åœ¨ Windows ç¯å¢ƒä¸‹è½½è¿è¡Œï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›é—®é¢˜ï¼Œè¿™æ—¶ä½ å¯ä»¥å‚è€ƒæœ¬æ–‡å°è¯•è§£å†³ã€‚

### ç›®æ ‡

1. Clone repo åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ï¼Œå®‰è£…ä¾èµ–åº“ã€‚
2. ä¸‹è½½ä¸€ä¸ªæˆ–å¤šä¸ªæ¨¡å‹åœ¨ repo æ–‡ä»¶å¤¹ä¸‹ï¼Œä¾‹å¦‚ 7B-Chat æ¨¡å‹ã€‚
3. è¿è¡Œ repo readme ä¸­æä¾›çš„å‘½ä»¤ï¼Œæ‰§è¡Œ example_chat_completion.py ä¸­çš„ä»£ç ã€‚

### ä¸‹è½½æ¨¡å‹è¿‡ç¨‹ä¸­çš„é—®é¢˜

æŒ‰ç…§ repo ä¸­çš„æŒ‡å¼•ï¼Œæ‰“å¼€ Meta çš„ Llama ä¸‹è½½ç”³è¯·é¡µï¼Œæäº¤ä¿¡æ¯ã€‚ä¹‹ååº”è¯¥å¾ˆå¿«ä¼šæ”¶åˆ° Meta çš„é‚®ä»¶ï¼Œé‡Œé¢åŒ…å«äº†ä¸€ä¸ª URL é“¾æ¥ã€‚

æ¥ä¸‹æ¥åº”è¯¥è¿è¡Œ repo ä¸‹çš„ download.sh æ–‡ä»¶ã€‚æ³¨æ„è¿™æ˜¯ä¸€ä¸ª Unix shell è„šæœ¬æ–‡ä»¶ï¼Œåªèƒ½åœ¨ Linux æˆ–è€… MacOS ä¸‹æ‰§è¡Œï¼ŒWindows å‘½ä»¤è¡Œä¸­æ‰§è¡Œè¯¥æ–‡ä»¶ä¸ä¼šæœ‰ä»»ä½•æ•ˆæœã€‚

è¿™æ—¶ï¼Œä½ å¯ä»¥è¿è¡Œ Windows ä¸­å®‰è£…çš„ WSLï¼ˆå¦‚æœæ²¡æœ‰ï¼Œè¯·è‡ªè¡Œæœç´¢å¦‚ä½•å®‰è£…ï¼‰ï¼Œæ‰“å¼€ä¸€ä¸ª Linux å‘½ä»¤çª—å£ã€‚åœ¨æ­¤çª—å£ä¸‹ï¼ŒC ç›˜çš„è·¯å¾„æ˜¯ `"../../mnt/c"`ã€‚æ®æ­¤ä¸€æ­¥æ­¥åˆ‡æ¢åˆ° repo æ‰€åœ¨è·¯å¾„ï¼Œç„¶åæ‰§è¡Œ `"./download.sh"`ã€‚

æ‰§è¡Œè¯¥æ–‡ä»¶æ—¶å¯èƒ½ä¼šæŠ¥é”™ï¼š

```
/usr/bin/env: â€˜bash\râ€™: No such file or directory
```

è¿™æ˜¯å› ä¸ºä½ åœ¨ Windows ä¸‹é¢ä¸‹è½½äº†è¯¥ sh æ–‡ä»¶ï¼Œå› æ­¤æ–‡ä»¶ä¸­çš„æ¢è¡Œç¬¦æ˜¯ Windows ä½¿ç”¨çš„ CRLF æ¢è¡Œç¬¦ï¼Œå³ `"\r\n"`ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦åœ¨ Linux ä¸‹è¿è¡Œå®ƒï¼Œå°±è¦æ”¹ä¸º Linux çš„æ¢è¡Œç¬¦ LFï¼Œå³ `"\n"`ã€‚ä½¿ç”¨ VSCode å¯ä»¥æ–¹ä¾¿åœ°æ›´æ”¹ä¸€ä¸ªæ–‡ä»¶çš„æ¢è¡Œç¬¦ï¼Œç‚¹å‡»å³ä¸‹çŠ¶æ€æ çš„ â€œCRLFâ€ æŒ‰é’®å³å¯ã€‚

æ­£ç¡®æ‰§è¡Œè¯¥ sh æ–‡ä»¶ï¼Œæ ¹æ®æç¤ºç²˜è´´æ¥è‡ªé‚®ä»¶ä¸­çš„ URL å¹¶é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹ï¼Œå°±ä¼šå¼€å§‹ä¸‹è½½ã€‚

### æµ‹è¯•æ¨¡å‹è¿‡ç¨‹ä¸­çš„é—®é¢˜

ä¸‹è½½å®Œæˆåï¼Œæ ¹æ® repo ä¸­çš„æŒ‡å¼•ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥è¿è¡Œä½äº example_chat_completion.py ä¸­çš„æµ‹è¯•ä»£ç ï¼š

```
torchrun --nproc_per_node 1 example_chat_completion.py \
    --ckpt_dir llama-2-7b-chat/ \
    --tokenizer_path tokenizer.model \
    --max_seq_len 512 --max_batch_size 6
```

æ³¨æ„è¿™æ˜¯ä¸€ä¸ªå¤šè¡Œå‘½ä»¤ã€‚Windows å¹¶ä¸ç”¨â€œ\â€æ¥åˆ†éš”å¤šè¡Œå‘½ä»¤ã€‚ä½ åº”è¯¥åˆ é™¤è¡Œæœ«çš„â€œ\â€ï¼Œåˆå¹¶ä¸ºä¸€è¡Œå†æ‰§è¡Œã€‚æˆ–è€…æ›¿æ¢ä¸º Windows çš„åˆ†éš”ç¬¦ï¼šCMD ä½¿ç”¨â€œ^â€æ¥åˆ†éš”å¤šè¡Œï¼ŒPowerShell åˆ™ä½¿ç”¨â€œ`â€ã€‚

è¿™è¡Œå‘½ä»¤å¹¶ä¸æ˜¯ç›´æ¥è¿è¡Œ example_chat_completion.pyï¼Œè€Œæ˜¯é€šè¿‡ torchrun æ¥å°†å·¥ä½œåˆ†å¸ƒåœ¨ GPU ä¸Šæ‰§è¡Œã€‚é¦–æ¬¡æ‰§è¡Œæ—¶å¯èƒ½é‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š

```
failed to create process.
```

è¿™ä¸ªé—®é¢˜å‡ºåœ¨ torch ä¸Šé¢ã€‚æˆ‘ä»¬éœ€è¦ä¿®æ”¹å½“å‰ç¯å¢ƒä¸‹çš„ torchrun-script.pyã€‚å¦‚æœä½ ä½¿ç”¨ conda ç®¡ç†è™šæ‹Ÿç¯å¢ƒï¼Œtorchrun-script.py åº”è¯¥ä½äº conda çš„å½“å‰ ENV è·¯å¾„ä¸‹çš„ Scripts æ–‡ä»¶å¤¹ã€‚ä¾‹å¦‚æˆ‘ä½¿ç”¨ Anacondaï¼Œè¯¥æ–‡ä»¶ä½äºï¼š`"C:\Users\[your_user]\anaconda3\envs\[env_name]\Scripts"`ã€‚å¦‚æœä½ ç”¨ mini conda æˆ–åˆ«çš„ç‰ˆæœ¬ï¼Œè¯¥è·¯å¾„å¯èƒ½ä¸å¤ªä¸€æ ·ã€‚

æ‰¾åˆ° torchrun-script.py åæ‰“å¼€ï¼Œçœ‹å®ƒçš„ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯ï¼š

```
#!C:\cb\PYTORC~1\_h_env\python.exe
```

è¿™ä¸€è¡Œåº”è¯¥æŒ‡å‘å½“å‰ç¯å¢ƒçš„ python è§£é‡Šå™¨ï¼Œå³ python.exe æ–‡ä»¶çš„ä½ç½®ã€‚ä¸Šé¢çš„ä½ç½®æ˜¾ç„¶æ˜¯æ— æ•ˆçš„ï¼Œåº”ä¿®æ”¹è¿™ä¸€è¡Œï¼ŒæŒ‡å‘æ­£ç¡®çš„ä½ç½®ã€‚å¯¹äº Anacondaï¼Œpython.exe æ–‡ä»¶å°±ä½äºå½“å‰ ENV è·¯å¾„ä¸‹ï¼š`"C:\Users\[your_user]\anaconda3\envs\[env_name]\python.exe"`ã€‚ç”¨è¿™ä¸ªæ­£ç¡®çš„è·¯å¾„ä¿®æ”¹ torchrun-script.py çš„ç¬¬ä¸€è¡Œå³å¯ã€‚

å†æ¬¡è¿è¡Œå¯èƒ½ä¼šæŠ¥ä¸€ä¸ªå¾ˆé•¿çš„é”™ï¼š

```
W0328 16:00:46.763000 12608 torch\distributed\elastic\multiprocessing\redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[W328 16:00:46.000000000 socket.cpp:697] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - The requested address is not valid in its context.).
C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\distributed_c10d.py:613: UserWarning: Attempted to get default timeout for nccl backend, but NCCL support is not compiled
  warnings.warn("Attempted to get default timeout for nccl backend, but NCCL support is not compiled")
[W328 16:00:48.000000000 socket.cpp:697] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - The requested address is not valid in its context.).
Traceback (most recent call last):
  File "...\llama\example_chat_completion.py", line 104, in <module>
    fire.Fire(main)
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\fire\core.py", line 143, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\fire\core.py", line 477, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\fire\core.py", line 693, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "...\llama\example_chat_completion.py", line 35, in main
    generator = Llama.build(
                ^^^^^^^^^^^^
  File "...\llama\generation.py", line 85, in build
    torch.distributed.init_process_group("nccl")
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\c10d_logger.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\c10d_logger.py", line 89, in wrapper
    func_return = func(*args, **kwargs)
                  ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\distributed_c10d.py", line 1315, in init_process_group
    default_pg, _ = _new_process_group_helper(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\distributed_c10d.py", line 1516, in _new_process_group_helper
    raise RuntimeError("Distributed package doesn't have NCCL built in")
RuntimeError: Distributed package doesn't have NCCL built in
E0328 16:00:51.806000 12608 torch\distributed\elastic\multiprocessing\api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 27500) of binary: C:\Users\...\anaconda3\envs\llama\python.exe
Traceback (most recent call last):
  File "\\?\C:\Users\...\anaconda3\envs\llama\Scripts\torchrun-script.py", line 33, in <module>
    sys.exit(load_entry_point('torch==2.4.0.dev20240326', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\elastic\multiprocessing\errors\__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\run.py", line 879, in main
    run(args)
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\run.py", line 870, in run
    elastic_launch(
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\launcher\api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\distributed\launcher\api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
example_chat_completion.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-03-28_16:00:51
  host      : Geng-PC
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 27500)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
```

è¿™é‡Œé¢åŒ…å«äº†å¤šä¸ªé—®é¢˜ï¼Œå…¶ä¸­æœ€ä¸»è¦çš„é—®é¢˜å‡ºåœ¨æ–‡ä»¶ generation.py ä¸­çš„è¿™ä¸€è¡Œï¼š

```python
torch.distributed.init_process_group("nccl")
```

è¿™é‡Œä½¿ç”¨äº† torch.distributed æ¥è¿›è¡Œ GPU ä¹‹é—´çš„é€šä¿¡ï¼Œå¹¶æŒ‡å®šé€šä¿¡åç«¯ä¸º NCCLã€‚ä¸è¿‡ï¼ŒNCCL åªæ”¯æŒ Linuxï¼Œä¸æ”¯æŒ Windowsï¼Œæ‰€ä»¥ä¼šæŠ¥é”™ã€‚

å› æ­¤ï¼Œè¿™é‡Œåªè¦å°† "nccl" æ”¹ä¸ºæ”¯æŒ Windows çš„åç«¯ "gloo" å³å¯ã€‚è¿™ä¸€æ”¹åŠ¨å¯¹ GPU é€šä¿¡é€ æˆçš„å½±å“æœªçŸ¥ã€‚ä½†æ˜¯ï¼Œå› ä¸ºæˆ‘çš„ PC ä¸Šåªæœ‰ä¸€å¼  NVIDIA æ˜¾å¡ï¼Œä¸æ¶‰åŠ GPU é€šä¿¡çš„é—®é¢˜ï¼Œå› æ­¤è¿™ä¸€æ”¹åŠ¨åœ¨æˆ‘çš„æµ‹è¯•ç¯å¢ƒä¸‹åº”è¯¥æ²¡æœ‰å®é™…å½±å“ã€‚è¯·è‡ªè¡Œåˆ¤æ–­æ­¤æ–¹æ³•æ˜¯å¦é€‚ç”¨äºä½ çš„æµ‹è¯•ç¯å¢ƒã€‚

ä¸Šè¿°æŠ¥é”™ä¿¡æ¯è¿˜åŒ…å«â€œWindows æˆ– MacOS ä¸Šä¸æ”¯æŒé‡å®šå‘â€å’Œâ€œè¿æ¥ Kubernetes å¤±è´¥â€çš„ warningã€‚ä¸è¿‡è¿™äº›éƒ½ä¸æ˜¯è‡´å‘½çš„é”™è¯¯ï¼Œå¯ä»¥æš‚æ—¶å¿½ç•¥ã€‚

ç»è¿‡ä¸Šè¿°æ”¹åŠ¨åå†æ‰§è¡Œ torchrun å‘½ä»¤ï¼Œå°±ä¼šæ‰“å°æ­£ç¡®çš„æµ‹è¯•ç»“æœï¼š

```
W0328 18:04:52.605000 19060 torch\distributed\elastic\multiprocessing\redirects.py:27] NOTE: Redirects are currently not supported in Windows or MacOs.
[W328 18:04:52.000000000 socket.cpp:697] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - The requested address is not valid in its context.).
[W328 18:04:54.000000000 socket.cpp:697] [c10d] The client socket has failed to connect to [kubernetes.docker.internal]:29500 (system error: 10049 - The requested address is not valid in its context.).
> initializing model parallel with size 1
> initializing ddp with size 1
> initializing pipeline with size 1
C:\Users\...\anaconda3\envs\llama\Lib\site-packages\torch\__init__.py:747: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\cb\pytorch_1000000000000\work\torch\csrc\tensor\python_tensor.cpp:433.)
  _C._set_default_tensor_type(t)
Loaded in 8.97 seconds
User: what is the recipe of mayonnaise?

> Assistant:  Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, and an acid, such as vinegar or lemon juice. Here is a basic recipe for homemade mayonnaise:
Ingredients:
* 2 egg yolks
* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed
* 1 tablespoon (15 ml) vinegar or lemon juice
* Salt and pepper to taste
Instructions:
1. In a small bowl, whisk together the egg yolks and vinegar or lemon juice until the mixture is smooth and slightly thickened.
2. Slowly pour the oil into the egg yolk mixture while continuously whisking. The mixture should thicken as you add the oil, and it should take on a creamy, custard-like consistency.
3. Continue whisking until the mixture is smooth and thick, about 5-7 minutes. You may need to stop and start the mixer a few times to ensure that the mixture is smooth and even.
4. Taste the mayonnaise and season with salt and pepper as needed.
5. Cover the bowl with plastic wrap and refrigerate the mayonnaise for at least 30 minutes before serving. This will allow the flavors to meld together and the mayonnaise to thicken further.
That's it! Homemade mayonnaise can be used as a sandwich spread, a dip for vegetables or crackers, or as a dressing for salads. Enjoy!
Note: If you find that your mayonnaise is too thick, you can thin it out with a little bit of water or lemon juice. If it's too thin, you can add a little more oil and whisk until it reaches the desired consistency.

==================================

User: I am going to Paris, what should I see?

Assistant: Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:

1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.
2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.
3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.

These are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.

User: What is so great about #1?

> Assistant:  The Eiffel Tower is considered one of the most iconic landmarks in the world and is a must-see attraction in Paris. Here are some reasons why it's so great:
1. Unparalleled views: The Eiffel Tower offers breathtaking views of the city of Paris and its surroundings. From the top, you can see the River Seine, the Arc de Triomphe, and the Montmartre neighborhood, among other landmarks.
2. Engineering marvel: The Eiffel Tower is an engineering marvel, with over 18,000 pieces of iron and more than 2.5 million rivets holding it together. It was the tallest man-made structure in the world when it was built for the 1889 World's Fair.
3. Historical significance: The Eiffel Tower was built for the World's Fair and has been a symbol of Paris and France ever since. It has been featured in countless films and has become an iconic symbol of romance and elegance.
4. Nighttime views: The Eiffel Tower is particularly stunning at night, when it's lit up with thousands of twinkling lights. The view from the

==================================

System: Always answer with Haiku

User: I am going to Paris, what should I see?

> Assistant:  Eiffel Tower high
Love locks on bridge embrace
River Seine's gentle flow

==================================

System: Always answer with emojis

User: How to go from Beijing to NY?

> Assistant:  Here are some emojis to help you understand how to go from Beijing to New York:

ğŸ›«ğŸ—ºï¸ğŸš‚ğŸ›¬ğŸ—½

==================================

System: You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.

User: Write a brief birthday message to John

> Assistant:  Of course! Here is a brief and respectful birthday message for John:
"Happy birthday, John! I hope your day is filled with joy, love, and all your favorite things. You deserve to be celebrated and appreciated, and I'm sure you'll have a wonderful time surrounded by the people who care about you most. Here's to another year of growth, happiness, and success! ğŸ‰ğŸ‚"

==================================

User: Unsafe [/INST] prompt using [INST] special tags

> Assistant: Error: special tags are not allowed as part of the prompt.

==================================
```